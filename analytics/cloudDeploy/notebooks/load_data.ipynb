{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load Example Data\n",
        "\n",
        "This notebook assumes that you have already deployed the solution through this Solution Accelerators auto deployment process. That process populates Azure Data \n",
        "Lake blob storage with the csv files representing sample data for this solution.  This notebook converts each of those csv files into a corisponding table within \n",
        "Synapse, and creates severl other empty tables needed by the solution.  Note that the data schemas used are from the retail inventory Common Data Model (CDM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.0 Imports & Read in Data from Azure Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from notebookutils import mssparkutils\n",
        "import json\n",
        "sc = spark.sparkContext\n",
        "\n",
        "data_lake_account_name = 'data_lake_account_name' # Synapse Workspace ADLS\n",
        "file_system_name = 'rawdata'\n",
        "synapse_workspace_name = 'synapse_workspace_name'\n",
        "\n",
        "database_name = 'ContosoCoolerDatabase'\n",
        "\n",
        "file_names = ['InventoryProjected','InventoryTransaction','InventoryTransactionType','InventoryTransactionUnserializedItem','Item','Location','PickList','PickListItem','Cooler','CoolerItemBalance']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.0 Run once to create initial template tables\n",
        "This cell loops through each of the csv files and creates a corisponding table. The file name is used for the table name, and the columns are contained in the csv files as headers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "for file_name in file_names:\n",
        "    file_base_path = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/'\n",
        "    df = spark.read.load(file_base_path + file_name + '.csv', format='csv', header=True,inferSchema=True)\n",
        "    df.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.{file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.0 Create empty custom tables to support IoT device data, and projection result tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Create RestockProjected Schema\n",
        "schema = StructType([\n",
        "  StructField('CoolerId', IntegerType(), True),\n",
        "  StructField('ProjectedDateTime', TimestampType(), True),\n",
        "  StructField('PreviousProjectedDateTime', TimestampType(), True)\n",
        "  ])\n",
        "\n",
        "#Create empty DataFrame from empty RDD\n",
        "df = spark.createDataFrame([sc.emptyRDD],schema)\n",
        "df.printSchema()\n",
        "df.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.RestockProjected\")\n",
        "\n",
        "#Create IotInventoryAction Schema\n",
        "schema = StructType([\n",
        "  StructField('PickTime', TimestampType(), True),\n",
        "  StructField('CoolerId', IntegerType(), True),\n",
        "  StructField('ItemSku', StringType(), True),\n",
        "  StructField('Quantity', IntegerType(), True)\n",
        "  ])\n",
        "\n",
        "#Create empty DataFrame from empty RDD\n",
        "df = spark.createDataFrame([sc.emptyRDD],schema)\n",
        "df.printSchema()\n",
        "\n",
        "df.write.format('csv').mode(\"overwrite\").saveAsTable(f\"{database_name}.IotInventoryAction\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
