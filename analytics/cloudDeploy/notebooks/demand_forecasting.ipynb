{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Demand forecasting and the [many models solution accelerator](https://github.com/Azure/azureml-examples/tree/main/python-sdk/tutorials/automl-with-azureml/forecasting-many-models)\n",
        "---\n",
        "This noebook takes the timeseries data sent from the coolers to the PickList and PickList items tables and prepares it for use in Azure Machine Learning. Once finished with \n",
        "the tasks in this notebook, you will pick up after the data preparation phase in the many models solution accelerator, to train forecasting models which will predict the \n",
        "number and type of items taken out of inventory at each individual cooler, allowing intelegent restocking, better decisions around the ballance of inventory to stock, etc.\n",
        "\n",
        "### Prerequisites \n",
        "At this point, you should have already:\n",
        "\n",
        "1. Deployed the solution\n",
        "1. Run the load_data notebook](./load_data.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.0 Initalization\n",
        "\n",
        "Here we initalize libraries and variables, as well as configuration of the storage we will be using (the default synapse workspace Data Lake) \n",
        "\n",
        "### Library Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-11T08:03:48.8426215Z",
              "execution_start_time": "2022-03-11T08:03:48.6830536Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-11T08:03:48.4497851Z",
              "session_id": 35,
              "session_start_time": null,
              "spark_pool": "conncoolerpool",
              "state": "finished",
              "statement_id": 10
            },
            "text/plain": [
              "StatementMeta(conncoolerpool, 35, 10, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "from notebookutils import mssparkutils\n",
        "import json\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Initialize variables and configure workspace storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-11T09:17:27.6830123Z",
              "execution_start_time": "2022-03-11T09:17:27.5430949Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-11T09:17:27.3938844Z",
              "session_id": 35,
              "session_start_time": null,
              "spark_pool": "conncoolerpool",
              "state": "finished",
              "statement_id": 41
            },
            "text/plain": [
              "StatementMeta(conncoolerpool, 35, 41, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "synapse_account_name = 'connected-cooler-sa-synapsews'\n",
        "data_lake_account_name = 'connectedcoolersasynadls' # Synapse Workspace ADLS\n",
        "file_system_name = 'connectedcoolersasynfs'\n",
        "ml_data_container = 'mldata'\n",
        "\n",
        "ml_directory = 'cooler-data'\n",
        "ds_train_path = 'ds-train'\n",
        "ds_inference_path = 'ds-inference'\n",
        "timestamp_split = '2021-09-15 00:00:00'\n",
        "\n",
        "t_file_path = f'abfss://{ml_data_container}@{data_lake_account_name}.dfs.core.windows.net/{ds_train_path}'\n",
        "I_file_path = f'abfss://{ml_data_container}@{data_lake_account_name}.dfs.core.windows.net/{ds_inference_path}'\n",
        "\n",
        "spark.conf.set(\"spark.storage.synapse.linkedServiceName\", f\"{synapse_account_name}-WorkspaceDefaultStorage\")\n",
        "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.0 Data Preparation\n",
        "In the Data Preparation notebook, we registered the orange juice inference data to the Workspace. You can choose to run the pipeline on the subet of 10 series or the full dataset of 11,973 series. We recommend starting with 10 series then expanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get data sets\n",
        "In this example, the datasets are stored in the blob storage setup with Synapse.  This allows us to use the existing linked service (setup above) for access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "picklist_df = spark.read.parquet(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/{database_name}/picklist')\n",
        "picklistitem_df = spark.read.parquet(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/{database_name}/picklistitem')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Join data and partition by cooler and sku, then split into training and inferencing data\n",
        "Join the PickList and PickListItem tables together.  This is the time series data showing number of items removed from the cooler per hour.  \n",
        "This is the data that will be useed to train our demand forecasting model.  The following code will organize the data into csv files by cooler and item. \n",
        "The power of the many models solution is that it will allow us to easily develop a model to predict sales for a particular item within a particualar cooler.\n",
        "In a production implementation of the solution, the cooler data could be augmented with other data such as local holidays, or weather data to help prediction accuracy.\n",
        "\n",
        "Next we split the training data from the inferencing test data.  You can control the split data by setting the timestamp_split variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-03-11T09:18:50.6187508Z",
              "execution_start_time": "2022-03-11T09:18:23.5214034Z",
              "livy_statement_state": "available",
              "queued_time": "2022-03-11T09:18:23.4073286Z",
              "session_id": 35,
              "session_start_time": null,
              "spark_pool": "conncoolerpool",
              "state": "finished",
              "statement_id": 42
            },
            "text/plain": [
              "StatementMeta(conncoolerpool, 35, 42, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# partition the data, then split it into training and inference sets\n",
        "for cooler in joined_df.select('CoolerId').distinct().collect():\n",
        "    for sku in joined_df.select('ItemSku').distinct().collect():\n",
        "        tmp_df = joined_df.where((joined_df.CoolerId == cooler['CoolerId']) & (joined_df.ItemSku == f\"{sku['ItemSku']}\"))\n",
        "        train_tmp_df = tmp_df.where(tmp_df.PickListFulfilledTimestamp <= timestamp_split) \\\n",
        "            .repartition(1) \\\n",
        "            .write \\\n",
        "            .option(\"header\",True) \\\n",
        "            .mode('append') \\\n",
        "            .csv(f\"{t_file_path}\")\n",
        "        infr_tmp_df = tmp_df.where(tmp_df.PickListFulfilledTimestamp > timestamp_split) \\\n",
        "            .repartition(1) \\\n",
        "            .write \\\n",
        "            .option(\"header\",True) \\\n",
        "            .mode('append') \\\n",
        "            .csv(f\"{I_file_path}\")        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### At this point, the data is configured and ready for training with the [Many Models Solution Accelerator](https://github.com/microsoft/solution-accelerator-many-models)."
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
